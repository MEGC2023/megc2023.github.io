<!DOCTYPE HTML>
<!--
	TXT by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
<head>
    <title>ACMMM MEGC2022</title>
    <meta charset="utf-8"/>
    <meta content="width=device-width, initial-scale=1, user-scalable=no" name="viewport"/>
    <link href="assets/css/main.css" rel="stylesheet"/>
</head>
<body class="is-preload">
<div id="page-wrapper">

    <!-- Header -->
    <header id="header">
        <div class="logo container">
            <div>
                <h1><a href="index.html" id="logo">ACMMM MEGC2022 </a></h1>
                <p>Facial Micro-Expression Grand Challenge 2022</p>
            </div>
        </div>
    </header>

    <!-- Nav -->
    <nav id="nav">
        <ul>
            <li><a href="index.html">Home</a></li>
            <li><a href="workshop.html">FME'22 Workshop</a>
                <ul>
                    <li><a href="workshop.html#agenda">Agenda</a></li>
                    <li><a href="workshop.html#submission">Submission</a></li>
                </ul>
            </li>
            <li
                    class="current"><a href="challenge.html">MEGC2022 Challenge</a>
                <ul>
                    <li><a href="challenge.html#genenration">Generations task</a></li>
                    <li><a href="challenge.html#spotting">Spotting task</a></li>
                    <li><a href="challenge.html#submission">Submission</a></li>
                    <li><a href="challenge.html#questions">Frequently Asked Questions</a></li>
                </ul>
            </li>
            <li><a href="program.html">Program</a></li>
            <li><a href="organisers.html">Organisers</a></li>
            <li><a href="review.html">Continuity</a></li>
        </ul>
    </nav>

    <!-- Main -->
    <section id="main">
        <div class="container">
            <section>
                <p><b>[update]</b>The Generation task ranking is available <a
                        href="challenge.html#generationRank">here</a>. The spotting task ranking is available <a
                        href="https://megc2022.grand-challenge.org/evaluation/challenge/leaderboard/">here</a>. </p>

                <p><b>Notification of Acceptance:</b>
                    <del>7 July 2022</del>
                    <span style="color: red; ">18 July 2022  (updated)</span></p>
                <p><b>[update]</b> The decision is based on the merit of the method and the performance (the
                    ranking).</p>
                <p><b>[update]</b> Supplementary file submission: Supplemental materials are <span style="color: red; ">required</span>
                    to be submitted on <a
                            href="https://openreview.net/group?id=acmmm.org/ACMMM/2022/Track/Grand_Challenges">OpenReview</a>.
                    See more submission requirement <a href="challenge.html#submission">here</a>. </p>
                <p><b>[update]</b>Submission platform:
                    <a href="https://openreview.net/group?id=acmmm.org/ACMMM/2022/Track/Grand_Challenges">https://openreview.net/group?id=acmmm.org/ACMMM/2022/Track/Grand_Challenges</a>.
                    See more submission requirement <a href="challenge.html#submission">here</a>.</p>

                <p><b>[update]</b> Submission deadline has been extended to <span style="color: red; ">25th June 2022 (23:59AoE)</span>
                </p>

                <p><b>[note]</b> Articles and results should be submitted <span style="color: red; ">together</span>,
                    both with a deadline of <span style="color: red; ">25th June 2022 (23:59AoE)</span>.
                </p>

                <p><b>[note for spotting task]</b> Challenge participants are allowed to create <span
                        style="color: red; ">5 submissions per day</span> on <b>Leaderboard</b> ：<a
                        href="https://megc2022.grand-challenge.org/">https://megc2022.grand-challenge.org/</a>.

            </section>
            <!-- Content -->
            <section>

                <header class="main">
                    <h2 class="major"><span>ME generation task</span></h2>
                </header>
                <a name="generation"></a>


                <!--<h2> - Advanced techniques for Facial Expressions Generation and Spotting</h2> -->
                <h3>Recommended Training Databases</h3>
                <uL>
                    <LI><b>SAMM with 159 MEs at 200fps.</b></Li>
                    <uL>
                        <li> To download the dataset, please visit: <a
                                href="http://www2.docm.mmu.ac.uk/STAFF/M.Yap/dataset.php">http://www2.docm.mmu.ac.uk/STAFF/M.Yap/dataset.php</a>.
                            Download and fill in the license agreement form, email to <a href="M.Yap@mmu.ac.uk">M.Yap@mmu.ac.uk</a>
                            with email subject: SAMM.
                        </li>
                        <li>
                            Reference: Davison, A. K., Lansley, C., Costen, N., Tan, K., & Yap, M. H. (2016). SAMM: A
                            spontaneous micro-facial movement dataset. IEEE transactions on affective computing, 9(1),
                            116-129.
                        </li>
                    </uL>


                    <LI><b>CASME II with 247 FMEs at 200 fps.</b></Li>
                    <uL>
                        <li> To download the dataset, please visit: <a href="http://fu.psych.ac.cn/CASME/casme2-en.php">http://fu.psych.ac.cn/CASME/casme2-en.php</a>.
                            Download and fill in the license agreement form, upload the file through this link: <a
                                    href="https://www.wjx.top/vj/hSaLoan.aspx">https://www.wjx.top/vj/hSaLoan.aspx</a>.
                        </li>
                        <li>
                            Reference: Yan, W. J., Li, X., Wang, S. J., Zhao, G., Liu, Y. J., Chen, Y. H., & Fu, X.
                            (2014). CASME II: An improved spontaneous micro-expression database and the baseline
                            evaluation. PloS one, 9(1), e86041.
                        </li>
                    </uL>

                    <LI><b>SMIC: SMIC-VIS and SMIC-NIR with 71 MEs at 25fps, SMIC-HS with 164 MEs at 100fps.</b>
                        <uL>
                            <li> To download the dataset, please visit: <a href="https://www.oulu.fi/cmvs/node/41319">https://www.oulu.fi/cmvs/node/41319</a>.
                                Download and fill in the license agreement form (please indicate which version/subset
                                you need), email to <a href="Xiaobai.Li@oulu.fi">Xiaobai.Li@oulu.fi</a>.
                            </li>
                            <li>
                                Reference: Li, X., Pfister, T., Huang, X., Zhao, G., & Pietikäinen, M. (2013, April). A
                                spontaneous micro-expression database: Inducement, collection and baseline. In 2013 10th
                                IEEE International Conference and Workshops on Automatic face and gesture recognition
                                (fg) (pp. 1-6). IEEE.
                            </li>
                        </uL>
                </uL>
                <p></p>
                <h3>Template faces and Designated micro-expressions for generation</h3>
                <LI> To download the MEGC2022-sythesis dataset, Download and fill in the <a
                        href="files/LA_megc2022_systhesis.pdf">license agreement form</a>, upload the file through this
                    link: <a href="https://www.wjx.top/vj/rWcxZXc.aspx">https://www.wjx.top/vj/rWcxZXc.aspx</a>.
                </li>


                <p></p>
                <h3>Evaluation Protocol</h3>
                <LI>Guidelines: Download <a href="files/MEGC2022_generation_guideline.pdf"> file here</a>.
                <LI>The three experts who are FACS AU coders will evaluate the results without interfering with each
                    other.

                    <p></p>
                    <h3>Evaluation Result</h3>  <a name="generationRank"></a>
                <LI>The submissions with the generated videos and the corresponding evaluation could be downloaded from
                    <a href="https://drive.google.com/drive/folders/1_6dVbrUufDMtdLbI7dRKwREx9h95mc-q?usp=sharing">
                        here</a>.
                <LI> The final evaluation on generated videos from three submissions


                    <style type="text/css">
                        .tg {
                            border-collapse: collapse;
                            border-spacing: 0;
                        }

                        .tg td {
                            border-color: black;
                            border-style: solid;
                            border-width: 1px;
                            font-family: Arial, sans-serif;
                            font-size: 14px;
                            overflow: hidden;
                            padding: 10px 5px;
                            word-break: normal;
                        }

                        .tg th {
                            border-color: black;
                            border-style: solid;
                            border-width: 1px;
                            font-family: Arial, sans-serif;
                            font-size: 14px;
                            font-weight: normal;
                            overflow: hidden;
                            padding: 10px 5px;
                            word-break: normal;
                        }

                        .tg .tg-8sni {
                            border-color: #333333;
                            text-align: center;
                            vertical-align: bottom
                        }

                        .tg .tg-j2c8 {
                            background-color: #f8ff00;
                            border-color: #333333;
                            font-weight: bold;
                            text-align: center;
                            vertical-align: bottom
                        }

                        .tg .tg-9ydz {
                            border-color: #333333;
                            font-weight: bold;
                            text-align: center;
                            vertical-align: top
                        }

                        .tg .tg-w3y9 {
                            border-color: #333333;
                            font-weight: bold;
                            text-align: center;
                            vertical-align: bottom
                        }
                    </style>
                    <table class="tg">
                        <thead>
                        <tr>
                            <th class="tg-9ydz" colspan="4">normalized/Perfect_score (9*36 =324)</th>
                        </tr>
                        </thead>
                        <tbody>
                        <tr>
                            <td class="tg-8sni"></td>
                            <td class="tg-8sni">#36</td>
                            <td class="tg-8sni">#46</td>
                            <td class="tg-8sni">#63</td>
                        </tr>
                        <tr>
                            <td class="tg-8sni">coder1</td>
                            <td class="tg-8sni">0.61</td>
                            <td class="tg-8sni">0.73</td>
                            <td class="tg-8sni">0.74</td>
                        </tr>
                        <tr>
                            <td class="tg-8sni">coder2</td>
                            <td class="tg-8sni">0.56</td>
                            <td class="tg-8sni">0.55</td>
                            <td class="tg-8sni">0.58</td>
                        </tr>
                        <tr>
                            <td class="tg-8sni">coder3</td>
                            <td class="tg-8sni">0.44</td>
                            <td class="tg-8sni">0.58</td>
                            <td class="tg-8sni">0.66</td>
                        </tr>
                        <tr>
                            <td class="tg-j2c8">total</td>
                            <td class="tg-j2c8">1.61</td>
                            <td class="tg-j2c8">1.86</td>
                            <td class="tg-j2c8">1.98</td>
                        </tr>
                        <tr>
                            <td class="tg-w3y9" colspan="4">normalized/max_submission_perCoder</td>
                        </tr>
                        <tr>
                            <td class="tg-8sni"></td>
                            <td class="tg-8sni">#36</td>
                            <td class="tg-8sni">#46</td>
                            <td class="tg-8sni">#63</td>
                        </tr>
                        <tr>
                            <td class="tg-8sni">coder1</td>
                            <td class="tg-8sni">0.83</td>
                            <td class="tg-8sni">1.00</td>
                            <td class="tg-8sni">1.00</td>
                        </tr>
                        <tr>
                            <td class="tg-8sni">coder2</td>
                            <td class="tg-8sni">0.97</td>
                            <td class="tg-8sni">0.95</td>
                            <td class="tg-8sni">1.00</td>
                        </tr>
                        <tr>
                            <td class="tg-8sni">coder3</td>
                            <td class="tg-8sni">0.66</td>
                            <td class="tg-8sni">0.88</td>
                            <td class="tg-8sni">1.00</td>
                        </tr>
                        <tr>
                            <td class="tg-j2c8">total</td>
                            <td class="tg-j2c8">2.46</td>
                            <td class="tg-j2c8">2.82</td>
                            <td class="tg-j2c8">3.00</td>
                        </tr>
                        </tbody>
                    </table>


            </section>
            <section>
                <header class="main">
                    <h2 class="major"><span>ME and Macro-expression Spotting task</span></h2>
                </header>
                <a name="spotting"></a>
                <h3>Recommended Training Databases</h3>
                <uL>
                    <LI><b>SAMM Long Videos with 147 long videos at 200 fps (average duration: 35.5s).</b>
                        <uL>
                            <li> To download the dataset, please visit: <a
                                    href="http://www2.docm.mmu.ac.uk/STAFF/M.Yap/dataset.php">http://www2.docm.mmu.ac.uk/STAFF/M.Yap/dataset.php</a>.
                                Download and fill in the license agreement form, email to <a href="M.Yap@mmu.ac.uk">M.Yap@mmu.ac.uk</a>
                                with email subject: SAMM long videos.
                            </li>
                            <li>
                                Reference: Yap, C. H., Kendrick, C., & Yap, M. H. (2020, November). SAMM long videos: A
                                spontaneous facial micro-and macro-expressions dataset. In 2020 15th IEEE International
                                Conference on Automatic Face and Gesture Recognition (FG 2020) (pp. 771-776). IEEE.
                            </li>
                        </uL>

                    <LI><b> CAS(ME)<sup>2</sup> with 97 long videos at 30 fps (average duration: 148s).</b>
                        <uL>
                            <li> To download the dataset, please visit: <a
                                    href="http://fu.psych.ac.cn/CASME/cas(me)2-en.php">http://fu.psych.ac.cn/CASME/casme2-en.php</a>.
                                Download and fill in the license agreement form, upload the file through this link: <a
                                        href="https://www.wjx.top/vj/QR147Sq.aspx">https://www.wjx.top/vj/QR147Sq.aspx</a>.
                            </li>
                            <li>
                                Reference: Qu, F., Wang, S. J., Yan, W. J., Li, H., Wu, S., & Fu, X. (2017). CAS (ME) $^
                                2$: a database for spontaneous macro-expression and micro-expression spotting and
                                recognition. IEEE Transactions on Affective Computing, 9(4), 424-436.
                            </li>

                        </uL>

                    <LI><b>SMIC-E-long with 162 long videos at 100 fps (average duration: 22s).</b>
                        <uL>
                            <li> To download the dataset, please visit: <a href="https://www.oulu.fi/cmvs/node/41319">https://www.oulu.fi/cmvs/node/41319</a>.
                                Download and fill in the license agreement form (please indicate which version/subset
                                you need), email to <a href="Xiaobai.Li@oulu.fi">Xiaobai.Li@oulu.fi</a>.
                            </li>
                            <li>
                                Reference: Tran, T. K., Vo, Q. N., Hong, X., Li, X., & Zhao, G. (2021). Micro-expression
                                spotting: A new benchmark. Neurocomputing, 443, 356-368.
                            </li>
                        </uL>
                </uL>
                <p></p>

                <h3>Unseen Test Dataset</h3>
                <uL>
                    <LI>The unseen testing set (MEGC2022-testSet) contains 10 long video, including 5 long videos from
                        SAMM
                        (SAMM Challenge dataset) and 5 clips cropped from different videos in CAS(ME)<sup>3</sup>. The
                        frame
                        rate for SAMM Challenge dataset is 200fps and the frame rate for CAS(ME)<sup>3</sup> is 30
                        fps. The participants should test on this unseen dataset.
                    <LI>To download the MEGC2022-testSet, Download and fill in the <a
                            href="files/SAMM_ReleaseAgreementV2.pdf">license agreement form of SAMM Challenge
                        dataset</a> and
                        the <a href="files/CAS3_clip_ReleaseAgreement.pdf">license agreement form of CAS(ME)<sup>3</sup>_clip</a>,
                        upload the file through this
                        link: <a href="https://www.wjx.top/vj/wMAN302.aspx">https://www.wjx.top/vj/wMAN302.aspx</a>.

                        <uL>
                            <li> For the request from a bank or company, the participants need to ask their director or
                                CEO to sign the form.
                            </li>
                            <li> Reference:
                                <ol>
                                    <li> Li, J., Dong, Z., Lu, S., Wang, S.J., Yan, W.J., Ma, Y., Liu, Y., Huang, C. and
                                        Fu, X. (2022). CAS(ME)<sup>3</sup>: A Third Generation Facial Spontaneous
                                        Micro-Expression Database with Depth Information and High Ecological Validity.
                                        IEEE
                                        Transactions on Pattern Analysis and Machine Intelligence, doi:
                                        10.1109/TPAMI.2022.3174895.
                                    </li>
                                    <li> Davison, A. K., Lansley, C., Costen, N., Tan, K., & Yap, M. H. (2016). SAMM: A
                                        spontaneous micro-facial movement dataset. IEEE transactions on affective
                                        computing, 9(1),
                                        116-129.
                                    </li>
                                </ol>

                            </li>


                        </uL>
                </LI>
                </uL>

                <p></p>
                <h3>Evaluation Protocol</h3>
                <uL>
                    <Li> Participant should test the proposed algorithm on the unseen dataset and upload the result
                        to the Leaderboard for the evaluation.
                    <Li> Baseline Method:<br/>
                        Please cite:
                        <br/>
                        Yap, C.H., Yap, M.H., Davison, A.K., Cunningham, R. (2021), 3D-CNN for Facial Micro-and
                        Macro-expression Spotting on Long Video Sequences using Temporal Oriented Reference Frame,
                        arXiv:2105.06340 [cs.CV], <a href="https://arxiv.org/abs/2105.06340">https://arxiv.org/abs/2105.06340</a>.
                    <Li> Baseline result: Available on the <a
                            href="https://megc2022.grand-challenge.org/evaluation/challenge/leaderboard/">Leaderboard</a>

                    <Li><b>Leaderboard</b> ：<a href="https://megc2022.grand-challenge.org/">https://megc2022.grand-challenge.org/</a>.
                        <ul>
                            <li> Please <a
                                    href="https://megc2022.grand-challenge.org/evaluation/challenge/submissions/create/">submit</a>
                                a <b>zip</b> file contains your predicted csv files with the following labels:
                                <ul>
                                    <li>cas_pred.csv
                                    <li>samm_pred.csv
                                </ul>
                            <li> An example submission can be seen at <a href="files/example_submission.zip">example_submission</a>
                                and <a href="files/example_submission_withoutExpType.zip">example_submission_withoutExpType</a>.

                            <li> Note: For submission without labelling expression type (me or mae), the labelling
                                will be done automatically using ME threshold of 0.5s (15 frames for CAS and 100
                                frames for SAMM).
                        </ul>

                    <Li><b>submission stage</b>: 23rd May -
                        <del>18th June</del>
                        <span style="color: red; "> 25th June 2022 (23:59AoE)  (updated)</span>
                        <ul>
                            <li> The participants could upload the result and then the Leaderboards will
                                calculate the metrics.
                            </li>
                            <li><b><font color="red">Update</font>: Please contact lijt@psych.ac.cn for the
                                participants' own evaluation
                                result, with mail subject: [Evaluation Result Request] MEGC2022 - Spotting task -
                                [user name] - [submission time].</b></li>
                            <li> The evaluation result of other participants and the ranking will not be provided
                                during this stage. You could compare your result with the provided baseline result.
                            </li>
                        </ul>
                    <Li><b>Live Leaderboard stage</b>: Since
                        <del>19th June 2022</del>
                        <span style="color: red; "> 26th June 2022  (updated)</span>
                        <ul>
                            <li> Results uploaded after June 25th will not be considered by ACM MEGC2022 for the
                                final ranking of the competition.
                            <li>However, any research team interested in the spotting task can upload results to
                                validate the performance of their method.
                            <li> The leaderboard will calculate and display the uploaded results and real-time
                                ranking.

                        </ul>


                </uL>


            </section>

            <section>
                <h2 class="major"><span>Submission</span></h2>
                <a name="submission"></a>
                <p> Please note: The submission deadline is at 11:59 p.m. of the stated deadline date <a
                        href="https://www.timeanddate.com/time/zones/aoe">Anywhere</a> on Earth.</p>
                <uL>
                    <Li><b>Submission platform: </b> <a
                            href="https://openreview.net/group?id=acmmm.org/ACMMM/2022/Track/Grand_Challenges">https://openreview.net/group?id=acmmm.org/ACMMM/2022/Track/Grand_Challenges.</a>
                    <Li><b>Submission Deadline:</b>
                        <del>18th June</del>
                        <span style="color: red; "> 25th June 2022 (updated)</span>
                    <Li><b>Notification:</b>
                        <del>7 July 2022</del>
                    <span style="color: red; ">18 July 2022  (updated)</span>
                    <Li><b>Camera-ready:</b>
                        <del>20 July 2022</del>
                        <span style="color: red; ">23 July 2022 (updated)</span>
                    <Li><b>Submission guidelines:</b>

                        <ul>
                            <li>Submitted papers (.pdf format) must use the ACM Article Template <a
                                    href="https://www.acm.org/publications/proceedings-template">https://www.acm.org/publications/proceedings-template</a>
                                as used by regular ACMMM submissions. Please use the template in traditional <b>double-column
                                    format</b> to prepare your submissions. For example, word users may use Word Interim
                                Template, and latex users may use <b>sample-sigconf</b> template.
                            <li>Grand challenge papers will go through a single-blind review process. Each grand
                                challenge paper
                                submission is limited to 4 pages with 1-2 extra pages for references only.
                            <li> For all files with different task requirements except for the paper, please submit in a
                                single zip file and upload to the submission system as supplementary material.
                                <ul>
                                    <li>GitHub repository URL containing codes of your implemented method, and all other
                                        relevant files such as feature/parameter data.
                                    <li> For <b>generation task</b>: the generated videos
                                    <li> For <b>spotting task</b>: two csv files reporting the results, i.e.,
                                        cas_pred.csv and samm_pred.csv.
                                </ul>
                        </uL>


                        <br/>
            </section>
            <section>
                <h2 class="major"><span>Frequently Asked Questions</span></h2>
                <a name="questions"></a>
                <OL>
                    <li>Q: How to deal with the spotted intervals with overlap? <br/>
                        A: We consider that each ground-truth interval corresponds to at most one single spotted
                        interval. If your algorithm detects multiple with overlap, you should merge them into an optimal
                        interval. The fusion method is also part of your algorithm, and the final result evaluation only
                        cares about the optimal interval obtained.
                    </li>


                </OL>


                <br/>
            </section>


        </div>
    </section>
    <footer id="footer">
        <!-- Copyright -->
        <div id="copyright">
            <ul class="menu">
                <li>GET IN TOUCH: lijt@psych.ac.cn</li>
                <li>&copy; MEGC2022. All rights reserved</li>
                <li>Design: <a href="http://html5up.net">HTML5 UP</a></li>
            </ul>
        </div>

    </footer>


</div>

<!-- Scripts -->
<!-- Scripts -->
<script src="assets/js/jquery.min.js"></script>
<script src="assets/js/jquery.dropotron.min.js"></script>
<script src="assets/js/jquery.scrolly.min.js"></script>
<script src="assets/js/browser.min.js"></script>
<script src="assets/js/breakpoints.min.js"></script>
<script src="assets/js/util.js"></script>
<script src="assets/js/main.js"></script>

</body>
</html>
